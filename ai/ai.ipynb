{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663083e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено файлов: 1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for summary:   0%|          | 0/1435 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for summary: 100%|██████████| 1435/1435 [01:03<00:00, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to: C:\\Users\\Professional\\Documents\\VirusNeutr\\parquet\\summary_clean.parquet\n",
      "Total rows before: 48487827, after cleaning: 48463283, removed: 24544\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# ----------------------\n",
    "# Настройки\n",
    "# ----------------------\n",
    "DATA_DIR = r\"C:\\Users\\Professional\\Documents\\VirusNeutr\\parquet\"\n",
    "PARQUET_PATTERN = os.path.join(DATA_DIR, \"*.parquet\")\n",
    "FILES = sorted(glob.glob(PARQUET_PATTERN))\n",
    "\n",
    "# Ограничения для cell-level набора (чтобы не перегружать память)\n",
    "MAX_TOTAL_CELLS_FOR_CELL_MODEL = 200_000\n",
    "MAX_CELLS_PER_SAMPLE = 3000\n",
    "\n",
    "# Random state\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Feature completeness threshold (fraction of samples that must have a non-null value)\n",
    "FEATURE_COMPLETE_THRESHOLD = 0.6  # 60% (настройка)\n",
    "\n",
    "# ----------------------\n",
    "# Функции помощники\n",
    "# ----------------------\n",
    "def drop_rows_with_nonfinite_numeric(df):\n",
    "    \"\"\"\n",
    "    Удаляет строки, где хотя бы в одном числовом столбце присутствует NaN/inf/-inf.\n",
    "    Возвращает очищенный df и количество удалённых строк.\n",
    "    \"\"\"\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(num_cols) == 0:\n",
    "        return df.copy(), 0\n",
    "    mask_finite = np.isfinite(df[num_cols].values).all(axis=1)\n",
    "    removed = (~mask_finite).sum()\n",
    "    return df.loc[mask_finite].copy(), int(removed)\n",
    "\n",
    "# ----------------------\n",
    "# 1) Сбор summary по всем файлам (с удалением строк с inf)\n",
    "# ----------------------\n",
    "summary_rows = []\n",
    "total_rows_before = 0\n",
    "total_rows_after = 0\n",
    "total_rows_removed = 0\n",
    "\n",
    "print(f\"Найдено файлов: {len(FILES)}\")\n",
    "for p in tqdm(FILES, desc=\"Processing files for summary\"):\n",
    "    try:\n",
    "        df = pd.read_parquet(p)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: не удалось прочитать {p}: {e}\")\n",
    "        continue\n",
    "\n",
    "    n_before = len(df)\n",
    "    total_rows_before += n_before\n",
    "\n",
    "    # Удаляем строки с inf/NaN в числовых колонках\n",
    "    df_clean, removed = drop_rows_with_nonfinite_numeric(df)\n",
    "    n_after = len(df_clean)\n",
    "    total_rows_after += n_after\n",
    "    total_rows_removed += removed\n",
    "\n",
    "    # соберём summary\n",
    "    summ = {\n",
    "        \"sample_file\": os.path.basename(p),\n",
    "        \"n_cells_original\": int(n_before),\n",
    "        \"n_cells_clean\": int(n_after),\n",
    "        \"n_rows_removed\": int(removed),\n",
    "    }\n",
    "\n",
    "    # процент инфицированных (если есть колонка label)\n",
    "    if \"label\" in df_clean.columns:\n",
    "        try:\n",
    "            summ[\"pct_infected_true\"] = float(100.0 * df_clean[\"label\"].astype(bool).mean())\n",
    "        except Exception:\n",
    "            summ[\"pct_infected_true\"] = None\n",
    "    else:\n",
    "        summ[\"pct_infected_true\"] = None\n",
    "\n",
    "    # статистики для всех числовых колонок на чистом df\n",
    "    num_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in num_cols:\n",
    "        series = df_clean[col]\n",
    "        if series.dropna().empty:\n",
    "            summ[f\"{col}_median\"] = None\n",
    "            summ[f\"{col}_mean\"] = None\n",
    "            summ[f\"{col}_mad\"] = None\n",
    "            summ[f\"{col}_p90\"] = None\n",
    "            summ[f\"{col}_p99\"] = None\n",
    "        else:\n",
    "            summ[f\"{col}_median\"] = float(series.median())\n",
    "            summ[f\"{col}_mean\"] = float(series.mean())\n",
    "            summ[f\"{col}_mad\"] = float((series - series.median()).abs().median())\n",
    "            summ[f\"{col}_p90\"] = float(series.quantile(0.90))\n",
    "            summ[f\"{col}_p99\"] = float(series.quantile(0.99))\n",
    "\n",
    "    summary_rows.append(summ)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = os.path.join(DATA_DIR, \"summary_clean.parquet\")\n",
    "summary_df.to_parquet(summary_path, index=False)\n",
    "print(f\"Summary saved to: {summary_path}\")\n",
    "print(f\"Total rows before: {total_rows_before}, after cleaning: {total_rows_after}, removed: {total_rows_removed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0861df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with true label: 1435\n",
      "Selected 88 features (threshold=0.60). Dropped 5 features due to low completeness.\n",
      "Example dropped features: ['FITC-H_median', 'FITC-H_mean', 'FITC-H_mad', 'FITC-H_p90', 'FITC-H_p99']\n",
      "Sample-level model results:\n",
      "  MAE: 0.6130846071769885\n",
      "  R2 : 0.9441285631194766\n",
      "Saved sample-level model to rf_sample_model_cleaned.joblib\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 2) Sample-level модель (изменённый блок: выбираем только \"полные\" фичи + impute)\n",
    "# ----------------------\n",
    "# Отберём строки с доступной целевой метрикой\n",
    "train_summary = summary_df.dropna(subset=[\"pct_infected_true\"]).copy()\n",
    "print(\"Samples with true label:\", len(train_summary))\n",
    "\n",
    "if len(train_summary) < 5:\n",
    "    print(\"Недостаточно образцов с true меткой для обучения sample-level модели. Пропускаем.\")\n",
    "else:\n",
    "    # Выбираем фичи: все колонки кроме sample_file и целевой\n",
    "    exclude = {\"sample_file\", \"pct_infected_true\"}\n",
    "    candidate_features = [c for c in train_summary.columns if c not in exclude]\n",
    "\n",
    "    # 2.1 Рассчитываем completeness для каждой candidate feature\n",
    "    completeness = train_summary[candidate_features].notna().mean(axis=0)\n",
    "    # оставляем те фичи, у которых completeness >= порога\n",
    "    thresh = FEATURE_COMPLETE_THRESHOLD\n",
    "    selected_features = completeness[completeness >= thresh].index.tolist()\n",
    "\n",
    "    # Если после фильтрации мало фич — понижаем порог автоматически, пока не найдём хотя бы 5 фич или не упадём до 0.1\n",
    "    min_features_needed = 5\n",
    "    min_thresh_allowed = 0.1\n",
    "    while len(selected_features) < min_features_needed and thresh > min_thresh_allowed:\n",
    "        thresh = max(thresh - 0.1, min_thresh_allowed)\n",
    "        selected_features = completeness[completeness >= thresh].index.tolist()\n",
    "\n",
    "    dropped_features = [f for f in candidate_features if f not in selected_features]\n",
    "    print(f\"Selected {len(selected_features)} features (threshold={thresh:.2f}). Dropped {len(dropped_features)} features due to low completeness.\")\n",
    "    if dropped_features:\n",
    "        print(\"Example dropped features:\", dropped_features[:10])\n",
    "\n",
    "    if len(selected_features) == 0:\n",
    "        print(\"Нет доступных фичей после фильтрации по полноте. Пропускаем обучение.\")\n",
    "    else:\n",
    "        # Формируем X из выбранных фич\n",
    "        X_df = train_summary[selected_features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Заполним NaN медианой каждой колонки\n",
    "        X_imputed = X_df.fillna(X_df.median())\n",
    "\n",
    "        # Убеждаемся, что остались только конечные значения\n",
    "        mask_finite = np.isfinite(X_imputed.values).all(axis=1)\n",
    "        n_dropped_rows = (~mask_finite).sum()\n",
    "        if n_dropped_rows > 0:\n",
    "            print(f\"Dropping {n_dropped_rows} rows with non-finite values even после импутации.\")\n",
    "        X_final = X_imputed.loc[mask_finite].reset_index(drop=True)\n",
    "        y_final = train_summary.loc[mask_finite, \"pct_infected_true\"].values.astype(float)\n",
    "\n",
    "        if X_final.shape[0] < 5:\n",
    "            print(\"После очистки и импутации слишком мало строк для обучения. Пропускаем.\")\n",
    "        else:\n",
    "            # Train/test split\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(X_final.values, y_final, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "            # Обучаем RandomForest\n",
    "            rf = RandomForestRegressor(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "            rf.fit(X_tr, y_tr)\n",
    "\n",
    "            y_pred = rf.predict(X_te)\n",
    "            print(\"Sample-level model results:\")\n",
    "            print(\"  MAE:\", mean_absolute_error(y_te, y_pred))\n",
    "            print(\"  R2 :\", r2_score(y_te, y_pred))\n",
    "\n",
    "            # Сохраняем модель и фичи\n",
    "            joblib.dump({\"model\": rf, \"feature_cols\": selected_features}, os.path.join(DATA_DIR, \"rf_sample_model_cleaned.joblib\"))\n",
    "            print(\"Saved sample-level model to rf_sample_model_cleaned.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a459d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_cells_original</th>\n",
       "      <th>n_cells_clean</th>\n",
       "      <th>n_rows_removed</th>\n",
       "      <th>FSC-H_median</th>\n",
       "      <th>FSC-H_mean</th>\n",
       "      <th>FSC-H_mad</th>\n",
       "      <th>FSC-H_p90</th>\n",
       "      <th>FSC-H_p99</th>\n",
       "      <th>SSC-H_median</th>\n",
       "      <th>SSC-H_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Area_to_Width_median</th>\n",
       "      <th>Area_to_Width_mean</th>\n",
       "      <th>Area_to_Width_mad</th>\n",
       "      <th>Area_to_Width_p90</th>\n",
       "      <th>Area_to_Width_p99</th>\n",
       "      <th>Time_norm_median</th>\n",
       "      <th>Time_norm_mean</th>\n",
       "      <th>Time_norm_mad</th>\n",
       "      <th>Time_norm_p90</th>\n",
       "      <th>Time_norm_p99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666</td>\n",
       "      <td>666</td>\n",
       "      <td>0</td>\n",
       "      <td>29596.0</td>\n",
       "      <td>1.011576e+05</td>\n",
       "      <td>23167.5</td>\n",
       "      <td>354817.5</td>\n",
       "      <td>521487.90</td>\n",
       "      <td>33548.5</td>\n",
       "      <td>70477.848348</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>654</td>\n",
       "      <td>654</td>\n",
       "      <td>0</td>\n",
       "      <td>28520.0</td>\n",
       "      <td>1.061099e+05</td>\n",
       "      <td>22276.0</td>\n",
       "      <td>362022.5</td>\n",
       "      <td>524272.00</td>\n",
       "      <td>22885.0</td>\n",
       "      <td>55081.481651</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>0</td>\n",
       "      <td>181051.5</td>\n",
       "      <td>1.835631e+05</td>\n",
       "      <td>40878.0</td>\n",
       "      <td>243697.2</td>\n",
       "      <td>291474.10</td>\n",
       "      <td>128241.0</td>\n",
       "      <td>157736.193900</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>0</td>\n",
       "      <td>175997.0</td>\n",
       "      <td>1.650866e+05</td>\n",
       "      <td>42580.0</td>\n",
       "      <td>238747.2</td>\n",
       "      <td>284554.07</td>\n",
       "      <td>122992.0</td>\n",
       "      <td>135971.194840</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>0</td>\n",
       "      <td>175647.5</td>\n",
       "      <td>1.692292e+05</td>\n",
       "      <td>42533.0</td>\n",
       "      <td>238151.1</td>\n",
       "      <td>283799.20</td>\n",
       "      <td>122293.0</td>\n",
       "      <td>143916.985400</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>37944</td>\n",
       "      <td>37862</td>\n",
       "      <td>82</td>\n",
       "      <td>4789083.0</td>\n",
       "      <td>4.769200e+06</td>\n",
       "      <td>713619.5</td>\n",
       "      <td>6149737.3</td>\n",
       "      <td>7687625.14</td>\n",
       "      <td>265554.0</td>\n",
       "      <td>295500.257012</td>\n",
       "      <td>...</td>\n",
       "      <td>31388.759884</td>\n",
       "      <td>31412.737088</td>\n",
       "      <td>5004.545591</td>\n",
       "      <td>41283.333654</td>\n",
       "      <td>51777.858414</td>\n",
       "      <td>0.458294</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>0.252038</td>\n",
       "      <td>0.908306</td>\n",
       "      <td>0.991570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>40000</td>\n",
       "      <td>39893</td>\n",
       "      <td>107</td>\n",
       "      <td>4800526.0</td>\n",
       "      <td>4.774798e+06</td>\n",
       "      <td>718477.0</td>\n",
       "      <td>6166968.0</td>\n",
       "      <td>7692920.84</td>\n",
       "      <td>263585.0</td>\n",
       "      <td>293657.929110</td>\n",
       "      <td>...</td>\n",
       "      <td>31638.723404</td>\n",
       "      <td>31647.468098</td>\n",
       "      <td>5083.157030</td>\n",
       "      <td>41707.107110</td>\n",
       "      <td>51880.123972</td>\n",
       "      <td>0.464877</td>\n",
       "      <td>0.487930</td>\n",
       "      <td>0.251127</td>\n",
       "      <td>0.904686</td>\n",
       "      <td>0.990898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>40000</td>\n",
       "      <td>39904</td>\n",
       "      <td>96</td>\n",
       "      <td>4831508.0</td>\n",
       "      <td>4.811184e+06</td>\n",
       "      <td>724216.0</td>\n",
       "      <td>6193953.3</td>\n",
       "      <td>7768334.40</td>\n",
       "      <td>264182.5</td>\n",
       "      <td>291829.083851</td>\n",
       "      <td>...</td>\n",
       "      <td>31867.325062</td>\n",
       "      <td>31949.631674</td>\n",
       "      <td>5147.668615</td>\n",
       "      <td>42052.228952</td>\n",
       "      <td>52465.496475</td>\n",
       "      <td>0.455257</td>\n",
       "      <td>0.479870</td>\n",
       "      <td>0.247533</td>\n",
       "      <td>0.901092</td>\n",
       "      <td>0.990797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>38145</td>\n",
       "      <td>38043</td>\n",
       "      <td>102</td>\n",
       "      <td>4846741.0</td>\n",
       "      <td>4.821535e+06</td>\n",
       "      <td>709524.0</td>\n",
       "      <td>6201318.4</td>\n",
       "      <td>7757935.32</td>\n",
       "      <td>263419.0</td>\n",
       "      <td>293500.605499</td>\n",
       "      <td>...</td>\n",
       "      <td>31758.390476</td>\n",
       "      <td>31809.718301</td>\n",
       "      <td>5064.701797</td>\n",
       "      <td>41914.880710</td>\n",
       "      <td>52446.521542</td>\n",
       "      <td>0.470803</td>\n",
       "      <td>0.495108</td>\n",
       "      <td>0.251720</td>\n",
       "      <td>0.912228</td>\n",
       "      <td>0.991377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>40000</td>\n",
       "      <td>39888</td>\n",
       "      <td>112</td>\n",
       "      <td>4870827.0</td>\n",
       "      <td>4.834638e+06</td>\n",
       "      <td>716553.5</td>\n",
       "      <td>6220675.4</td>\n",
       "      <td>7668283.62</td>\n",
       "      <td>264065.0</td>\n",
       "      <td>291096.265167</td>\n",
       "      <td>...</td>\n",
       "      <td>32099.310461</td>\n",
       "      <td>32101.316786</td>\n",
       "      <td>5158.700256</td>\n",
       "      <td>42378.148143</td>\n",
       "      <td>52356.990895</td>\n",
       "      <td>0.468270</td>\n",
       "      <td>0.491701</td>\n",
       "      <td>0.252948</td>\n",
       "      <td>0.909090</td>\n",
       "      <td>0.991481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1435 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_cells_original  n_cells_clean  n_rows_removed  FSC-H_median  \\\n",
       "0                  666            666               0       29596.0   \n",
       "1                  654            654               0       28520.0   \n",
       "2                50000          50000               0      181051.5   \n",
       "3                50000          50000               0      175997.0   \n",
       "4                50000          50000               0      175647.5   \n",
       "...                ...            ...             ...           ...   \n",
       "1430             37944          37862              82     4789083.0   \n",
       "1431             40000          39893             107     4800526.0   \n",
       "1432             40000          39904              96     4831508.0   \n",
       "1433             38145          38043             102     4846741.0   \n",
       "1434             40000          39888             112     4870827.0   \n",
       "\n",
       "        FSC-H_mean  FSC-H_mad  FSC-H_p90   FSC-H_p99  SSC-H_median  \\\n",
       "0     1.011576e+05    23167.5   354817.5   521487.90       33548.5   \n",
       "1     1.061099e+05    22276.0   362022.5   524272.00       22885.0   \n",
       "2     1.835631e+05    40878.0   243697.2   291474.10      128241.0   \n",
       "3     1.650866e+05    42580.0   238747.2   284554.07      122992.0   \n",
       "4     1.692292e+05    42533.0   238151.1   283799.20      122293.0   \n",
       "...            ...        ...        ...         ...           ...   \n",
       "1430  4.769200e+06   713619.5  6149737.3  7687625.14      265554.0   \n",
       "1431  4.774798e+06   718477.0  6166968.0  7692920.84      263585.0   \n",
       "1432  4.811184e+06   724216.0  6193953.3  7768334.40      264182.5   \n",
       "1433  4.821535e+06   709524.0  6201318.4  7757935.32      263419.0   \n",
       "1434  4.834638e+06   716553.5  6220675.4  7668283.62      264065.0   \n",
       "\n",
       "         SSC-H_mean  ...  Area_to_Width_median  Area_to_Width_mean  \\\n",
       "0      70477.848348  ...                   NaN                 NaN   \n",
       "1      55081.481651  ...                   NaN                 NaN   \n",
       "2     157736.193900  ...                   NaN                 NaN   \n",
       "3     135971.194840  ...                   NaN                 NaN   \n",
       "4     143916.985400  ...                   NaN                 NaN   \n",
       "...             ...  ...                   ...                 ...   \n",
       "1430  295500.257012  ...          31388.759884        31412.737088   \n",
       "1431  293657.929110  ...          31638.723404        31647.468098   \n",
       "1432  291829.083851  ...          31867.325062        31949.631674   \n",
       "1433  293500.605499  ...          31758.390476        31809.718301   \n",
       "1434  291096.265167  ...          32099.310461        32101.316786   \n",
       "\n",
       "      Area_to_Width_mad  Area_to_Width_p90  Area_to_Width_p99  \\\n",
       "0                   NaN                NaN                NaN   \n",
       "1                   NaN                NaN                NaN   \n",
       "2                   NaN                NaN                NaN   \n",
       "3                   NaN                NaN                NaN   \n",
       "4                   NaN                NaN                NaN   \n",
       "...                 ...                ...                ...   \n",
       "1430        5004.545591       41283.333654       51777.858414   \n",
       "1431        5083.157030       41707.107110       51880.123972   \n",
       "1432        5147.668615       42052.228952       52465.496475   \n",
       "1433        5064.701797       41914.880710       52446.521542   \n",
       "1434        5158.700256       42378.148143       52356.990895   \n",
       "\n",
       "      Time_norm_median  Time_norm_mean  Time_norm_mad  Time_norm_p90  \\\n",
       "0                  NaN             NaN            NaN            NaN   \n",
       "1                  NaN             NaN            NaN            NaN   \n",
       "2                  NaN             NaN            NaN            NaN   \n",
       "3                  NaN             NaN            NaN            NaN   \n",
       "4                  NaN             NaN            NaN            NaN   \n",
       "...                ...             ...            ...            ...   \n",
       "1430          0.458294        0.486180       0.252038       0.908306   \n",
       "1431          0.464877        0.487930       0.251127       0.904686   \n",
       "1432          0.455257        0.479870       0.247533       0.901092   \n",
       "1433          0.470803        0.495108       0.251720       0.912228   \n",
       "1434          0.468270        0.491701       0.252948       0.909090   \n",
       "\n",
       "      Time_norm_p99  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "...             ...  \n",
       "1430       0.991570  \n",
       "1431       0.990898  \n",
       "1432       0.990797  \n",
       "1433       0.991377  \n",
       "1434       0.991481  \n",
       "\n",
       "[1435 rows x 88 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76edf8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting cells for cell-level model (dropping rows with inf/NaN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting cells:   5%|▍         | 68/1435 [00:00<00:18, 74.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected approx. 202320 cells from 69 files.\n",
      "After final cleaning, cell dataset shape: (202320, 15) (removed 0 more rows)\n",
      "Cell-level feature matrix: (202320, 14), positives: 8075, negatives: 194245\n",
      "Cell-level model results:\n",
      "  Accuracy: 0.9980476472914195\n",
      "  ROC AUC : 0.9992816754111956\n",
      "Saved cell-level model to rf_cell_model_cleaned.joblib\n",
      "\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 3) Cell-level модель (если есть label)\n",
    "#    Собираем подпоследовательности клеток из файлов, предварительно отбрасывая строки с inf\n",
    "# ----------------------\n",
    "cell_frames = []\n",
    "collected = 0\n",
    "print(\"\\nCollecting cells for cell-level model (dropping rows with inf/NaN)...\")\n",
    "for p in tqdm(FILES, desc=\"Collecting cells\"):\n",
    "    try:\n",
    "        df = pd.read_parquet(p)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: не удалось прочитать {p}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"label\" not in df.columns:\n",
    "        continue\n",
    "\n",
    "    # очистка строк с inf/NaN\n",
    "    df_clean, removed = drop_rows_with_nonfinite_numeric(df)\n",
    "    if df_clean.empty:\n",
    "        continue\n",
    "\n",
    "    # подвыборка, чтобы не захватить слишком много от одного файла\n",
    "    take = min(len(df_clean), MAX_CELLS_PER_SAMPLE)\n",
    "    sample_part = df_clean.sample(n=take, random_state=RANDOM_STATE) if len(df_clean) > take else df_clean\n",
    "    cell_frames.append(sample_part)\n",
    "    collected += len(sample_part)\n",
    "    if collected >= MAX_TOTAL_CELLS_FOR_CELL_MODEL:\n",
    "        break\n",
    "\n",
    "print(f\"Collected approx. {collected} cells from {len(cell_frames)} files.\")\n",
    "\n",
    "if collected < 100:\n",
    "    print(\"Недостаточно размеченных клеток для обучения cell-level модели. Пропускаем.\")\n",
    "else:\n",
    "    cell_all = pd.concat(cell_frames, ignore_index=True)\n",
    "    # ещё раз удалить строки с нечисловыми в числовых колонках (страховка)\n",
    "    cell_all, removed2 = drop_rows_with_nonfinite_numeric(cell_all)\n",
    "    print(f\"After final cleaning, cell dataset shape: {cell_all.shape} (removed {removed2} more rows)\")\n",
    "\n",
    "    # Формируем фичи и таргет\n",
    "    if \"label\" not in cell_all.columns:\n",
    "        print(\"В собранных данных нет label — пропускаем cell-level модель.\")\n",
    "    else:\n",
    "        y_cell = cell_all[\"label\"].astype(int).values\n",
    "        # используем все числовые колонки кроме label\n",
    "        feat_cols_cell = [c for c in cell_all.select_dtypes(include=[np.number]).columns if c != \"label\"]\n",
    "        X_cell_df = cell_all[feat_cols_cell].apply(pd.to_numeric, errors='coerce')\n",
    "        # отбрасываем строки с NaN/inf (если остались)\n",
    "        mask_finite_cell = np.isfinite(X_cell_df.values).all(axis=1)\n",
    "        X_cell_df = X_cell_df.loc[mask_finite_cell].reset_index(drop=True)\n",
    "        y_cell = y_cell[mask_finite_cell]\n",
    "\n",
    "        print(f\"Cell-level feature matrix: {X_cell_df.shape}, positives: {y_cell.sum()}, negatives: {len(y_cell)-y_cell.sum()}\")\n",
    "\n",
    "        if X_cell_df.shape[0] < 100:\n",
    "            print(\"После очистки слишком мало клеток для обучения — пропускаем.\")\n",
    "        else:\n",
    "            # Train/test\n",
    "            Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(\n",
    "                X_cell_df.values, y_cell, test_size=0.2, random_state=RANDOM_STATE, stratify=y_cell\n",
    "            )\n",
    "\n",
    "            clf = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1, class_weight=\"balanced\")\n",
    "            clf.fit(Xc_tr, yc_tr)\n",
    "\n",
    "            yc_pred = clf.predict(Xc_te)\n",
    "            yc_prob = clf.predict_proba(Xc_te)[:, 1]\n",
    "\n",
    "            print(\"Cell-level model results:\")\n",
    "            print(\"  Accuracy:\", accuracy_score(yc_te, yc_pred))\n",
    "            try:\n",
    "                print(\"  ROC AUC :\", roc_auc_score(yc_te, yc_prob))\n",
    "            except Exception as e:\n",
    "                print(\"  ROC AUC : couldn't compute:\", e)\n",
    "\n",
    "            joblib.dump({\"model\": clf, \"features\": feat_cols_cell}, os.path.join(DATA_DIR, \"rf_cell_model_cleaned.joblib\"))\n",
    "            print(\"Saved cell-level model to rf_cell_model_cleaned.joblib\")\n",
    "\n",
    "print(\"\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec5b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5fff4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f435eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8dcf503",
   "metadata": {},
   "source": [
    "# 2nd way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad9d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 09:43:11,478 INFO Loading parquet into memory: E:\\\\parquet\\\\merged_processed.parquet\n",
      "2025-12-02 09:43:19,421 INFO Loaded shape: (48463283, 15)\n",
      "2025-12-02 09:43:19,422 INFO No sample-id column detected — will perform stratified split by label (possible leakage).\n",
      "2025-12-02 09:43:22,614 INFO Dropped column: FITC-H\n",
      "2025-12-02 09:43:53,234 INFO Using 13 features. Example: ['FITC-A', 'FITC_asinh', 'FITC_log10', 'FITC_pct', 'FITC_raw', 'FITC_z_robust', 'FSC-A', 'FSC-H', 'FSC_ratio', 'SSC-A']\n",
      "2025-12-02 09:43:53,286 INFO Total rows=48463283, positives=3520061, positive rate=0.072634\n",
      "2025-12-02 09:43:57,152 INFO Built X shape=(48463283, 13), y shape=(48463283,)\n",
      "2025-12-02 09:44:11,049 INFO Stratified split done.\n",
      "2025-12-02 09:44:17,590 INFO Train shape: (41193790, 13), Val shape: (7269493, 13)\n",
      "2025-12-02 09:44:17,614 INFO Train pos rate=0.072634 Val pos rate=0.072634\n",
      "2025-12-02 09:44:17,636 INFO scale_pos_weight = 12.768\n",
      "2025-12-02 09:44:17,637 INFO Start lgb.train with params: {'learning_rate': 0.05, 'num_leaves': 127, 'scale_pos_weight': 12.767738662295976}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's auc: 0.993302\tvalid's auc: 0.993309\n",
      "[200]\ttrain's auc: 0.995023\tvalid's auc: 0.994988\n",
      "[300]\ttrain's auc: 0.995685\tvalid's auc: 0.995618\n",
      "[400]\ttrain's auc: 0.996097\tvalid's auc: 0.995992\n",
      "[500]\ttrain's auc: 0.996407\tvalid's auc: 0.996266\n",
      "[600]\ttrain's auc: 0.99663\tvalid's auc: 0.996457\n",
      "[700]\ttrain's auc: 0.996828\tvalid's auc: 0.996632\n",
      "[800]\ttrain's auc: 0.997008\tvalid's auc: 0.996783\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 215\u001b[0m\n\u001b[0;32m    212\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 160\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callbacks:\n\u001b[1;32m--> 160\u001b[0m         booster \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_BOOST_ROUND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m         booster \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(params, dtrain,\n\u001b[0;32m    167\u001b[0m                             num_boost_round\u001b[38;5;241m=\u001b[39mNUM_BOOST_ROUND,\n\u001b[0;32m    168\u001b[0m                             valid_sets\u001b[38;5;241m=\u001b[39m[dtrain, dval],\n\u001b[0;32m    169\u001b[0m                             valid_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\engine.py:282\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m--> 282\u001b[0m         evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    283\u001b[0m     evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_valid(feval))\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\basic.py:4125\u001b[0m, in \u001b[0;36mBooster.eval_train\u001b[1;34m(self, feval)\u001b[0m\n\u001b[0;32m   4093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_train\u001b[39m(\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4095\u001b[0m     feval: Optional[Union[_LGBM_CustomEvalFunction, List[_LGBM_CustomEvalFunction]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_LGBM_BoosterEvalMethodResultType]:\n\u001b[0;32m   4097\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for training data.\u001b[39;00m\n\u001b[0;32m   4098\u001b[0m \n\u001b[0;32m   4099\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4123\u001b[0m \u001b[38;5;124;03m        List with (train_dataset_name, eval_name, eval_result, is_higher_better) tuples.\u001b[39;00m\n\u001b[0;32m   4124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_data_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\basic.py:4848\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[0;32m   4846\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_inner_eval, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   4847\u001b[0m tmp_out_len \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 4848\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterGetEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_out_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   4853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmp_out_len\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_inner_eval:\n\u001b[0;32m   4854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of eval results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_in_memory_fixed.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GroupShuffleSplit, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ---------------- USER CONFIG ----------------\n",
    "PARQUET_PATH = r\"E:\\\\parquet\\\\merged_processed.parquet\"\n",
    "MODEL_OUT = r\"E:\\\\parquet\\\\lgbm_inmemory_booster.joblib\"\n",
    "TEST_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "NUM_BOOST_ROUND = 2000\n",
    "VERBOSE = 100   # how often to log eval during training\n",
    "DROP_COLS = ['FITC-H']  # drop FITC-H as requested\n",
    "# base lgb params (will add scale_pos_weight later)\n",
    "LGB_PARAMS = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 127,\n",
    "    \"verbosity\": -1,\n",
    "    \"n_jobs\": -1,\n",
    "    # 'metric' will be set in train call via valid_sets (or we can set here)\n",
    "    \"metric\": \"auc\"\n",
    "}\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Logging\n",
    "logger = logging.getLogger(\"train_in_memory_fixed\")\n",
    "logger.setLevel(logging.INFO)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "logger.addHandler(ch)\n",
    "\n",
    "def detect_sample_id_column(df):\n",
    "    candidates = ['sample_file','sample','sample_id','filename','file','orig_file','source_file']\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        s = str(c).lower()\n",
    "        if 'sample' in s or 'file' in s or 'filename' in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def prepare_data(df, drop_cols=None, label_col='label'):\n",
    "    df = df.loc[:, ~df.columns.str.startswith('__')].copy()\n",
    "\n",
    "    if drop_cols:\n",
    "        for c in drop_cols:\n",
    "            if c in df.columns:\n",
    "                df.drop(columns=[c], inplace=True)\n",
    "                logger.info(f\"Dropped column: {c}\")\n",
    "\n",
    "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if label_col in numeric:\n",
    "        numeric.remove(label_col)\n",
    "    feature_cols = sorted([c for c in numeric if not str(c).startswith(\"__\")])\n",
    "\n",
    "    if label_col not in df.columns:\n",
    "        logger.warning(f\"Label column '{label_col}' not found — creating zeros.\")\n",
    "        df[label_col] = 0\n",
    "\n",
    "    lbl = df[label_col]\n",
    "    if lbl.dtype == 'bool':\n",
    "        df[label_col] = lbl.astype(int)\n",
    "    else:\n",
    "        df[label_col] = lbl.replace({'True':1,'TRUE':1,'true':1,'False':0,'FALSE':0,'false':0})\n",
    "        df[label_col] = pd.to_numeric(df[label_col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    for c in feature_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    medians = df[feature_cols].median()\n",
    "    df[feature_cols] = df[feature_cols].fillna(medians)\n",
    "\n",
    "    return df, feature_cols\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading parquet into memory: %s\", PARQUET_PATH)\n",
    "    df = pd.read_parquet(PARQUET_PATH)\n",
    "    logger.info(\"Loaded shape: %s\", df.shape)\n",
    "\n",
    "    sample_col = detect_sample_id_column(df)\n",
    "    if sample_col is not None:\n",
    "        logger.info(\"Detected sample-id column: %s — group-aware split will be used.\", sample_col)\n",
    "    else:\n",
    "        logger.info(\"No sample-id column detected — will perform stratified split by label (possible leakage).\")\n",
    "\n",
    "    df, feature_cols = prepare_data(df, drop_cols=DROP_COLS, label_col='label')\n",
    "    logger.info(\"Using %d features. Example: %s\", len(feature_cols), feature_cols[:10])\n",
    "\n",
    "    n_total = len(df)\n",
    "    n_pos = int(df['label'].sum())\n",
    "    pos_rate = n_pos / max(1, n_total)\n",
    "    logger.info(\"Total rows=%d, positives=%d, positive rate=%.6f\", n_total, n_pos, pos_rate)\n",
    "\n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    y = df['label'].values.astype(np.int8)\n",
    "    logger.info(\"Built X shape=%s, y shape=%s\", X.shape, y.shape)\n",
    "\n",
    "    # split\n",
    "    if sample_col is not None:\n",
    "        groups = df[sample_col].values\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "        train_idx, val_idx = next(splitter.split(X, y, groups))\n",
    "        logger.info(\"GroupShuffleSplit done.\")\n",
    "    else:\n",
    "        if len(np.unique(y)) > 1 and np.min(np.bincount(y)) > 1:\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "            train_idx, val_idx = next(sss.split(X, y))\n",
    "            logger.info(\"Stratified split done.\")\n",
    "        else:\n",
    "            train_idx, val_idx = train_test_split(np.arange(len(y)), test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "            logger.info(\"Random split done.\")\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    logger.info(\"Train shape: %s, Val shape: %s\", X_train.shape, X_val.shape)\n",
    "    logger.info(\"Train pos rate=%.6f Val pos rate=%.6f\", y_train.mean(), y_val.mean())\n",
    "\n",
    "    # compute scale_pos_weight\n",
    "    n_pos_tr = int(y_train.sum())\n",
    "    n_neg_tr = len(y_train) - n_pos_tr\n",
    "    if n_pos_tr == 0:\n",
    "        scale_pos_weight = 1.0\n",
    "        logger.warning(\"No positives in train: scale_pos_weight set to 1.0\")\n",
    "    else:\n",
    "        scale_pos_weight = max(1.0, n_neg_tr / max(1.0, n_pos_tr))\n",
    "    logger.info(\"scale_pos_weight = %.3f\", scale_pos_weight)\n",
    "\n",
    "    # prepare lgb datasets\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train, feature_name=feature_cols)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, reference=dtrain, feature_name=feature_cols)\n",
    "\n",
    "    # update params with scale_pos_weight\n",
    "    params = dict(LGB_PARAMS)\n",
    "    params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "    # callbacks: try to use early stopping / logging; if not available fallback to training without callbacks\n",
    "    callbacks = []\n",
    "    try:\n",
    "        callbacks.append(lgb.early_stopping(EARLY_STOPPING_ROUNDS))\n",
    "        callbacks.append(lgb.log_evaluation(VERBOSE))\n",
    "    except Exception as e:\n",
    "        logger.warning(\"LightGBM callbacks not available or failed to create: %s. Will still try training.\", e)\n",
    "        callbacks = None\n",
    "\n",
    "    logger.info(\"Start lgb.train with params: %s\", {k: params[k] for k in ['learning_rate','num_leaves','scale_pos_weight'] if k in params})\n",
    "    try:\n",
    "        if callbacks:\n",
    "            booster = lgb.train(params, dtrain,\n",
    "                                num_boost_round=NUM_BOOST_ROUND,\n",
    "                                valid_sets=[dtrain, dval],\n",
    "                                valid_names=['train','valid'],\n",
    "                                callbacks=callbacks)\n",
    "        else:\n",
    "            booster = lgb.train(params, dtrain,\n",
    "                                num_boost_round=NUM_BOOST_ROUND,\n",
    "                                valid_sets=[dtrain, dval],\n",
    "                                valid_names=['train','valid'])\n",
    "    except TypeError as e:\n",
    "        logger.warning(\"lgb.train() raised TypeError: %s — trying without callbacks/valid_sets\", e)\n",
    "        booster = lgb.train(params, dtrain, num_boost_round=NUM_BOOST_ROUND)\n",
    "\n",
    "    # predictions on validation\n",
    "    logger.info(\"Predicting on validation set...\")\n",
    "    probs_val = booster.predict(X_val, num_iteration=booster.best_iteration or None)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_val, probs_val)\n",
    "    except Exception:\n",
    "        roc = float('nan')\n",
    "    try:\n",
    "        pr = average_precision_score(y_val, probs_val)\n",
    "    except Exception:\n",
    "        pr = float('nan')\n",
    "\n",
    "    preds_val = (probs_val >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_val, preds_val)\n",
    "    prec = precision_score(y_val, preds_val, zero_division=0)\n",
    "    rec = recall_score(y_val, preds_val, zero_division=0)\n",
    "    f1 = f1_score(y_val, preds_val, zero_division=0)\n",
    "    cm = confusion_matrix(y_val, preds_val)\n",
    "\n",
    "    logger.info(\"Validation metrics: ROC AUC=%.6f, PR AUC=%.6f\", roc, pr)\n",
    "    logger.info(\"Val acc=%.6f precision=%.6f recall=%.6f f1=%.6f\", acc, prec, rec, f1)\n",
    "    logger.info(\"Confusion matrix:\\n%s\", cm)\n",
    "\n",
    "    # feature importance by gain\n",
    "    try:\n",
    "        names = booster.feature_name()\n",
    "        gains = booster.feature_importance(importance_type='gain')\n",
    "        fi = sorted(zip(names, gains), key=lambda x: x[1], reverse=True)\n",
    "        logger.info(\"Top features by gain:\")\n",
    "        for name, g in fi[:20]:\n",
    "            logger.info(\"  %s: %f\", name, g)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Couldn't extract feature importances: %s\", e)\n",
    "\n",
    "    # save booster + feature list\n",
    "    logger.info(\"Saving booster model and feature list to: %s\", MODEL_OUT)\n",
    "    joblib.dump({'booster': booster, 'features': feature_cols}, MODEL_OUT)\n",
    "\n",
    "    logger.info(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb18159",
   "metadata": {},
   "source": [
    "# 3d way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02429eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 11:43:25,236 - INFO - Notebook training script started\n"
     ]
    }
   ],
   "source": [
    "# %%cell\n",
    "# Конфигурация + импорты\n",
    "PARQUET_PATH = r\"E:\\\\parquet\\\\merged_processed.parquet\"  # <- ваш путь\n",
    "LABEL_COL = \"label\"\n",
    "DROP_COL = \"FITC-H\"\n",
    "\n",
    "# Настройки для потоковой обработки / подвыборки\n",
    "VAL_FRAC = 0.05           # доля для валидации (при полном чтении) или размер валидации при потоковой (как доля)\n",
    "MAX_VAL_SAMPLES_PER_CLASS = 200_000  # ограничение для валидационного набора на класс в режиме стрима\n",
    "TRAIN_SUBSAMPLE_FRAC = 0.1  # если памяти нет, можно обучаться на подвыборке 10% (если выбрано)\n",
    "\n",
    "# Параметры LightGBM (можно править)\n",
    "LGB_PARAMS = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_bin': 127,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'num_threads': 31,   # подберите под вашу машину\n",
    "    'verbose': -1,\n",
    "    'device': 'gpu',\n",
    "}\n",
    "\n",
    "# Импорты\n",
    "import os, time, math, tempfile, joblib, logging, tracemalloc\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import psutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Optional libs: polars (fast parquet), pyarrow (stream)\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"polars required: pip install polars\") from e\n",
    "\n",
    "try:\n",
    "    import pyarrow.dataset as ds\n",
    "    import pyarrow as pa\n",
    "except Exception:\n",
    "    ds = None\n",
    "\n",
    "# ML libs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "# Logging\n",
    "LOGFILE = \"training_notebook.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler(LOGFILE, mode='a')]\n",
    ")\n",
    "logger = logging.getLogger(\"flow_training\")\n",
    "logger.info(\"Notebook training script started\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88e8c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 11:43:25,253 - INFO - System memory: {'total_GB': 68.623040512, 'available_GB': 58.087292928, 'used_GB': 10.535747584}\n"
     ]
    }
   ],
   "source": [
    "# %%cell\n",
    "# Утилиты\n",
    "\n",
    "def memory_info():\n",
    "    mem = psutil.virtual_memory()\n",
    "    return {\"total_GB\": mem.total / 1e9, \"available_GB\": mem.available / 1e9, \"used_GB\": mem.used / 1e9}\n",
    "\n",
    "def estimate_memory_needed(n_rows, n_cols, bytes_per_value=4, overhead_factor=6.0):\n",
    "    \"\"\"\n",
    "    Очень грубая оценка памяти (в байтах) для pandas/polars в памяти.\n",
    "    bytes_per_value: 4 for float32\n",
    "    overhead_factor: эмпирический множитель (pandas сильнее расходует память)\n",
    "    \"\"\"\n",
    "    base = n_rows * n_cols * bytes_per_value\n",
    "    return base * overhead_factor / 1e9  # в GB\n",
    "\n",
    "def df_preview(path, n=5):\n",
    "    # быстрый preview через polars\n",
    "    return pl.read_parquet(path, n_rows=n).to_pandas()\n",
    "\n",
    "logger.info(f\"System memory: {memory_info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc88b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 11:43:25,402 - INFO - Estimated rows: 48463283, columns: ['FSC-H', 'SSC-H', 'FITC-H', 'FSC-A', 'SSC-A', 'FITC-A', 'label', 'FITC_raw', 'FITC_log10', 'FITC_asinh', 'FITC_pct', 'FITC_z_robust', 'FSC_ratio', 'SSC_ratio', 'gmm_prob_infected', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "# %%cell\n",
    "# Быстрая оценка числа строк и столбцов без загрузки всего файла\n",
    "# Polars предоставляет scanning metadata, или используем pyarrow dataset\n",
    "\n",
    "def get_parquet_shape(path):\n",
    "    try:\n",
    "        # polars scan to get height (fast metadata read)\n",
    "        lf = pl.scan_parquet(path)\n",
    "        # materialize schema and n_rows\n",
    "        # polars currently can't give n_rows directly from scan, so fallback to pyarrow\n",
    "    except Exception:\n",
    "        lf = None\n",
    "    # fallback via pyarrow\n",
    "    if ds is not None:\n",
    "        dataset = ds.dataset(path, format=\"parquet\")\n",
    "        n_rows = 0\n",
    "        cols = None\n",
    "        for frag in dataset.get_fragments():\n",
    "            try:\n",
    "                md = frag.metadata\n",
    "                n_rows += md.num_rows\n",
    "            except Exception:\n",
    "                # fallback: smaller cost - read fragment table\n",
    "                pass\n",
    "        # columns via sampling small read\n",
    "        sample = pl.read_parquet(path, n_rows=1)\n",
    "        cols = sample.columns\n",
    "        return int(n_rows) if n_rows>0 else None, cols\n",
    "    else:\n",
    "        # worst case read tiny chunk\n",
    "        tmp = pl.read_parquet(path, n_rows=10)\n",
    "        return None, tmp.columns\n",
    "\n",
    "n_rows_est, cols = get_parquet_shape(PARQUET_PATH)\n",
    "logger.info(f\"Estimated rows: {n_rows_est}, columns: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bb316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 11:43:25,418 - INFO - Estimated memory needed (GB): 18.61, available (GB): 58.04\n",
      "2025-12-02 11:43:25,419 - INFO - USE_FULL_LOAD = True\n"
     ]
    }
   ],
   "source": [
    "# %%cell\n",
    "# Решение: если хватает памяти — full load, иначе streaming fallback\n",
    "\n",
    "# получим точное число строк, если polars может быстро посчитать\n",
    "try:\n",
    "    # polars can compute n_rows via read_parquet with n_rows=0 ? => we'll try reading statistics\n",
    "    # but if file is large, better to avoid full scan; rely on earlier estimate if available\n",
    "    if n_rows_est is None:\n",
    "        # attempt one fast metadata call via polars (may still be inexpensive)\n",
    "        n_rows_est = pl.read_parquet(PARQUET_PATH, n_rows=0).shape[0]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "n_cols = len(cols) if cols is not None else 15\n",
    "if n_rows_est is None:\n",
    "    # fallback assume large (conservative)\n",
    "    n_rows_est = 48_463_283\n",
    "\n",
    "# estimated memory needed (GB)\n",
    "est_needed_gb = estimate_memory_needed(n_rows_est, n_cols, bytes_per_value=4, overhead_factor=6.0)\n",
    "avail = memory_info()['available_GB']\n",
    "logger.info(f\"Estimated memory needed (GB): {est_needed_gb:.2f}, available (GB): {avail:.2f}\")\n",
    "\n",
    "USE_FULL_LOAD = avail > est_needed_gb * 1.5  # safety margin\n",
    "\n",
    "logger.info(f\"USE_FULL_LOAD = {USE_FULL_LOAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "773092a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 11:47:03,363 - INFO - Loading full parquet into memory with Polars...\n",
      "2025-12-02 11:47:04,961 - INFO - Loaded Polars DF: rows=48463283, cols=16 in 1.6s\n",
      "2025-12-02 11:47:04,964 - INFO - Dropped column FITC-H\n",
      "2025-12-02 11:47:06,834 - INFO - Converted to pandas DataFrame (in-memory)\n",
      "2025-12-02 11:47:28,765 - INFO - Train size: (46040118, 13), Val size: (2423165, 13)\n",
      "2025-12-02 11:47:28,849 - INFO - Starting LightGBM training (in-memory). GPU auto-detect not enforced here.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's auc: 0.988644\tvalid's auc: 0.988651\n",
      "[100]\ttrain's auc: 0.990668\tvalid's auc: 0.990658\n",
      "[150]\ttrain's auc: 0.991816\tvalid's auc: 0.991814\n",
      "[200]\ttrain's auc: 0.992409\tvalid's auc: 0.992389\n",
      "[250]\ttrain's auc: 0.992837\tvalid's auc: 0.992807\n",
      "[300]\ttrain's auc: 0.993151\tvalid's auc: 0.993124\n",
      "[350]\ttrain's auc: 0.993377\tvalid's auc: 0.993366\n",
      "[400]\ttrain's auc: 0.993578\tvalid's auc: 0.993565\n",
      "[450]\ttrain's auc: 0.993561\tvalid's auc: 0.993546\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# callbacks: early stopping + periodic logging + record evals into dict\u001b[39;00m\n\u001b[0;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [lgb\u001b[38;5;241m.\u001b[39mearly_stopping(\u001b[38;5;241m100\u001b[39m), lgb\u001b[38;5;241m.\u001b[39mlog_evaluation(\u001b[38;5;241m50\u001b[39m), lgb\u001b[38;5;241m.\u001b[39mrecord_evaluation(evals_result)]\n\u001b[1;32m---> 54\u001b[0m gbm \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLGB_PARAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLightGBM finished in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min. Best iter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(gbm,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_iteration\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    269\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m    270\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    271\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m    272\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[0;32m    273\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[0;32m    274\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 276\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\basic.py:3891\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3890\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 3891\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3892\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\basic.py:263\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n"
     ]
    }
   ],
   "source": [
    "# %%cell\n",
    "# Full load + LightGBM training (исправленная версия)\n",
    "if USE_FULL_LOAD:\n",
    "    t0 = time.time()\n",
    "    logger.info(\"Loading full parquet into memory with Polars...\")\n",
    "    df_pl = pl.read_parquet(PARQUET_PATH)\n",
    "    logger.info(f\"Loaded Polars DF: rows={df_pl.height}, cols={len(df_pl.columns)} in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "    # Drop column FITC-H if present\n",
    "    if DROP_COL in df_pl.columns:\n",
    "        df_pl = df_pl.drop(DROP_COL)\n",
    "        logger.info(f\"Dropped column {DROP_COL}\")\n",
    "    \n",
    "    # Cast numeric columns to float32 where possible, label to int8\n",
    "    def cast_float32(pl_df, label_col=LABEL_COL):\n",
    "        cols = pl_df.columns\n",
    "        casts = []\n",
    "        for c in cols:\n",
    "            if c == label_col:\n",
    "                casts.append(pl.col(c).cast(pl.Int8).alias(c))\n",
    "            else:\n",
    "                # Попытка кастить все остальные колонки в Float32 (если это невозможно, результат будет null)\n",
    "                casts.append(pl.col(c).cast(pl.Float32).alias(c))\n",
    "        return pl_df.with_columns(casts)\n",
    "    \n",
    "    df_pl = cast_float32(df_pl, LABEL_COL)\n",
    "    \n",
    "    # Convert to pandas for sklearn/lightgbm (polars -> pandas uses memory; but we already checked memory)\n",
    "    df = df_pl.to_pandas()\n",
    "    # df = df.dropna()\n",
    "    logger.info(\"Converted to pandas DataFrame (in-memory)\")\n",
    "    \n",
    "    # Split features/labels\n",
    "    X = df.drop(columns=[LABEL_COL, \"__index_level_0__\"])\n",
    "    y = df[LABEL_COL].astype(int)\n",
    "    del df_pl  # free polars object if still referenced\n",
    "\n",
    "    # Stratified split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_FRAC, stratify=y, random_state=42)\n",
    "    logger.info(f\"Train size: {X_train.shape}, Val size: {X_val.shape}\")\n",
    "    \n",
    "    # Optionally subsample training for faster experiments\n",
    "    # X_train, y_train = sklearn.utils.shuffle(X_train, y_train, random_state=42)\n",
    "    \n",
    "    # Train LightGBM\n",
    "    if LGB_AVAILABLE:\n",
    "        lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "        evals_result = {}\n",
    "        logger.info(\"Starting LightGBM training (in-memory). GPU auto-detect not enforced here.\")\n",
    "        t0 = time.time()\n",
    "        # callbacks: early stopping + periodic logging + record evals into dict\n",
    "        callbacks = [lgb.early_stopping(100), lgb.log_evaluation(50), lgb.record_evaluation(evals_result)]\n",
    "        gbm = lgb.train(\n",
    "            LGB_PARAMS,\n",
    "            lgb_train,\n",
    "            num_boost_round=2000,\n",
    "            valid_sets=[lgb_train, lgb_val],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        logger.info(f\"LightGBM finished in {(time.time()-t0)/60:.2f} min. Best iter: {getattr(gbm, 'best_iteration', None)}\")\n",
    "        # Save model\n",
    "        model_path = \"lgb_model_inmemory.txt\"\n",
    "        gbm.save_model(model_path)\n",
    "        logger.info(f\"Saved model to {model_path}\")\n",
    "        \n",
    "        # Eval metrics\n",
    "        y_pred = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        y_bin = (y_pred >= 0.5).astype(int)\n",
    "        acc = accuracy_score(y_val, y_bin)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_val, y_bin, average='binary')\n",
    "        logger.info(f\"Val metrics - AUC: {auc:.5f}, Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "        \n",
    "        # Save training history (if recorded)\n",
    "        try:\n",
    "            train_auc = evals_result['train']['auc']\n",
    "            valid_auc = evals_result['valid']['auc']\n",
    "            hist_df = pd.DataFrame({'train_auc': train_auc, 'valid_auc': valid_auc})\n",
    "            hist_df.to_csv(\"lgb_training_history.csv\", index=False)\n",
    "            logger.info(\"Saved training history to lgb_training_history.csv\")\n",
    "        except Exception:\n",
    "            logger.warning(\"Could not record evals_result history; skipping save.\")\n",
    "        \n",
    "        # SHAP explanation on a small sample (expensive) - run only on subset\n",
    "        try:\n",
    "            import shap\n",
    "            sample_idx = np.random.choice(X_val.shape[0], size=min(2000, X_val.shape[0]), replace=False)\n",
    "            explainer = shap.TreeExplainer(gbm)\n",
    "            shap_values = explainer.shap_values(X_val.iloc[sample_idx])\n",
    "            # plot summary (in notebook)\n",
    "            shap.summary_plot(shap_values, X_val.iloc[sample_idx], show=True)\n",
    "            logger.info(\"SHAP summary plotted for a sample of validation set\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"SHAP not executed: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"LightGBM not available. Consider pip install lightgbm or use fallback SGD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f305148",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input must be categorical",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m canvas \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mCanvas(plot_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, plot_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# аггрегация\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m agg \u001b[38;5;241m=\u001b[39m \u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFSC-A\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSSC-A\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_cat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# shading через color_key для категориальных меток\u001b[39;00m\n\u001b[0;32m     15\u001b[0m color_key \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datashader\\core.py:229\u001b[0m, in \u001b[0;36mCanvas.points\u001b[1;34m(self, source, x, y, agg, geometry)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource must be an instance of spatialpandas.GeoDataFrame, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdask_geopandas.GeoDataFrame. Received objects of type \u001b[39m\u001b[38;5;132;01m{typ}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    227\u001b[0m                 typ\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(source)))\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbypixel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglyph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datashader\\core.py:1345\u001b[0m, in \u001b[0;36mbypixel\u001b[1;34m(source, canvas, glyph, agg, antialias)\u001b[0m\n\u001b[0;32m   1343\u001b[0m schema \u001b[38;5;241m=\u001b[39m dshape\u001b[38;5;241m.\u001b[39mmeasure\n\u001b[0;32m   1344\u001b[0m glyph\u001b[38;5;241m.\u001b[39mvalidate(schema)\n\u001b[1;32m-> 1345\u001b[0m \u001b[43magg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1346\u001b[0m canvas\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;66;03m# All-NaN objects (e.g. chunks of arrays with no data) are valid in Datashader\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datashader\\reductions.py:751\u001b[0m, in \u001b[0;36mby.validate\u001b[1;34m(self, in_dshape)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_dshape):\n\u001b[1;32m--> 751\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_dshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction\u001b[38;5;241m.\u001b[39mvalidate(in_dshape)\n",
      "File \u001b[1;32mc:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datashader\\reductions.py:161\u001b[0m, in \u001b[0;36mcategory_codes.validate\u001b[1;34m(self, in_dshape)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified column not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(in_dshape\u001b[38;5;241m.\u001b[39mmeasure[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn], ct\u001b[38;5;241m.\u001b[39mCategorical):\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must be categorical\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: input must be categorical"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# df = sns.load_dataset(\"penguins\")\n",
    "sns.pairplot(df[:30000], hue=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477f7aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datashader\n",
      "  Downloading datashader-0.18.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting colorcet (from datashader)\n",
      "  Downloading colorcet-3.1.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting multipledispatch (from datashader)\n",
      "  Downloading multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting numba (from datashader)\n",
      "  Downloading numba-0.62.1-cp312-cp312-win_amd64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datashader) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datashader) (23.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datashader) (2.2.1)\n",
      "Collecting param (from datashader)\n",
      "  Downloading param-2.3.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pyct (from datashader)\n",
      "  Downloading pyct-0.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datashader) (2.32.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datashader) (1.12.0)\n",
      "Collecting toolz (from datashader)\n",
      "  Downloading toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting xarray (from datashader)\n",
      "  Downloading xarray-2025.11.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba->datashader)\n",
      "  Downloading llvmlite-0.45.1-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datashader) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datashader) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datashader) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datashader) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->datashader) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->datashader) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->datashader) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\professional\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->datashader) (2024.2.2)\n",
      "Collecting packaging (from datashader)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading datashader-0.18.2-py3-none-any.whl (18.3 MB)\n",
      "   ---------------------------------------- 0.0/18.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.6/18.3 MB 6.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 6.6/18.3 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 10.2/18.3 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 11.8/18.3 MB 14.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.8/18.3 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.3/18.3 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading colorcet-3.1.0-py3-none-any.whl (260 kB)\n",
      "Downloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading numba-0.62.1-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 22.6 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.45.1-cp312-cp312-win_amd64.whl (38.1 MB)\n",
      "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.7/38.1 MB 19.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 4.5/38.1 MB 10.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 13.6/38.1 MB 21.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.9/38.1 MB 24.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 24.9/38.1 MB 23.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 30.7/38.1 MB 24.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 36.4/38.1 MB 24.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.1/38.1 MB 24.7 MB/s eta 0:00:00\n",
      "Downloading param-2.3.1-py3-none-any.whl (139 kB)\n",
      "Downloading pyct-0.6.0-py3-none-any.whl (16 kB)\n",
      "Downloading toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "Downloading xarray-2025.11.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: multipledispatch, toolz, param, packaging, llvmlite, colorcet, pyct, numba, xarray, datashader\n",
      "\n",
      "   ---- -----------------------------------  1/10 [toolz]\n",
      "   -------- -------------------------------  2/10 [param]\n",
      "   -------- -------------------------------  2/10 [param]\n",
      "  Attempting uninstall: packaging\n",
      "   -------- -------------------------------  2/10 [param]\n",
      "    Found existing installation: packaging 23.2\n",
      "   -------- -------------------------------  2/10 [param]\n",
      "    Uninstalling packaging-23.2:\n",
      "   -------- -------------------------------  2/10 [param]\n",
      "      Successfully uninstalled packaging-23.2\n",
      "   -------- -------------------------------  2/10 [param]\n",
      "   ------------ ---------------------------  3/10 [packaging]\n",
      "   ---------------- -----------------------  4/10 [llvmlite]\n",
      "   ---------------- -----------------------  4/10 [llvmlite]\n",
      "   ---------------- -----------------------  4/10 [llvmlite]\n",
      "   ---------------- -----------------------  4/10 [llvmlite]\n",
      "   -------------------- -------------------  5/10 [colorcet]\n",
      "   ------------------------ ---------------  6/10 [pyct]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   ---------------------------- -----------  7/10 [numba]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   -------------------------------- -------  8/10 [xarray]\n",
      "   ------------------------------------ ---  9/10 [datashader]\n",
      "   ------------------------------------ ---  9/10 [datashader]\n",
      "   ------------------------------------ ---  9/10 [datashader]\n",
      "   ------------------------------------ ---  9/10 [datashader]\n",
      "   ------------------------------------ ---  9/10 [datashader]\n",
      "   ------------------------------------ ---  9/10 [datashader]\n",
      "   ------------------------------------ ---  9/10 [datashader]\n",
      "   ---------------------------------------- 10/10 [datashader]\n",
      "\n",
      "Successfully installed colorcet-3.1.0 datashader-0.18.2 llvmlite-0.45.1 multipledispatch-1.0.0 numba-0.62.1 packaging-25.0 param-2.3.1 pyct-0.6.0 toolz-1.1.0 xarray-2025.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datashader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589079b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8db507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1435 parquet files. Merging into C:\\\\Users\\\\Professional\\\\Documents\\\\VirusNeutr\\\\parquet\\merged.parquet ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging files: 100%|██████████| 1435/1435 [01:05<00:00, 22.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging finished. total rows scanned: 48487827, rows written: 48463283, rows removed: 24544\n",
      "Merged columns (sample): ['FSC-H', 'SSC-H', 'FITC-H', 'FSC-A', 'SSC-A', 'FITC-A', 'label', 'FITC_raw', 'FITC_log10', 'FITC_asinh', 'FITC_pct', 'FITC_z_robust', 'FSC_ratio', 'SSC_ratio', 'gmm_prob_infected', '__index_level_0__']\n",
      "Cell-level label present?: True\n",
      "Building cell-level training dataset (subsampling across files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling files for cell-level:   4%|▍         | 62/1435 [00:00<00:19, 71.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell-level dataset shape: (300000, 14) positive rate: 0.03927666666666667\n",
      "Cell-level accuracy: 0.99825\n",
      "Cell-level ROC AUC: 0.9999309569719919\n",
      "Saved cell-level model to C:\\\\Users\\\\Professional\\\\Documents\\\\VirusNeutr\\\\parquet\\cell_level_rf.joblib\n",
      "Building sample-level summary from each original parquet file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing sample summaries: 100%|██████████| 1435/1435 [00:46<00:00, 30.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample summary to C:\\\\Users\\\\Professional\\\\Documents\\\\VirusNeutr\\\\parquet\\summary_from_merged.parquet\n",
      "Samples with true target: 1435\n",
      "Sample-level MAE: 0.6913409797441538\n",
      "Sample-level R2: 0.9365949133907789\n",
      "Saved sample-level model to C:\\\\Users\\\\Professional\\\\Documents\\\\VirusNeutr\\\\parquet\\sample_level_rf.joblib\n",
      "Running inference over all files and producing Excel + plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 1435/1435 [02:27<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel summary to C:\\\\Users\\\\Professional\\\\Documents\\\\VirusNeutr\\\\parquet\\analysis_summary.xlsx\n",
      "Saved plots to C:\\\\Users\\\\Professional\\\\Documents\\\\VirusNeutr\\\\parquet\\plots\n",
      "Pipeline finished successfully.\n"
     ]
    }
   ],
   "source": [
    "# analyze_folder.py\n",
    "\"\"\"\n",
    "End-to-end pipeline for flow cytometry parquet files:\n",
    " - merge parquet files into one merged.parquet (streaming, pyarrow)\n",
    " - clean rows with non-finite numeric values\n",
    " - (optionally) add simple derived features if missing (FITC_log10, FITC_asinh, ratios)\n",
    " - build cell-level classifier (if 'label' exists)\n",
    " - build sample-level regressor (if sample-level true percent exists in files)\n",
    " - run inference for all files and save summary Excel + plots\n",
    " - save models and metadata (feature lists, scaler)\n",
    " \n",
    "Requirements:\n",
    "  pip install pandas pyarrow scikit-learn joblib matplotlib tqdm openpyxl\n",
    "Run:\n",
    "  python analyze_folder.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# USER CONFIGURATION\n",
    "# -----------------------\n",
    "DATA_DIR = r\"C:\\\\Users\\\\Professional\\\\Documents\\\\VirusNeutr\\\\parquet\"   # <- папка с parquet\n",
    "MERGED_PATH = os.path.join(DATA_DIR, \"merged.parquet\")\n",
    "SUMMARY_PATH = os.path.join(DATA_DIR, \"summary_from_merged.parquet\")\n",
    "EXCEL_OUT = os.path.join(DATA_DIR, \"analysis_summary.xlsx\")\n",
    "CELL_MODEL_PATH = os.path.join(DATA_DIR, \"cell_level_rf.joblib\")\n",
    "SAMPLE_MODEL_PATH = os.path.join(DATA_DIR, \"sample_level_rf.joblib\")\n",
    "PLOTS_DIR = os.path.join(DATA_DIR, \"plots\")\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Limits to avoid OOM\n",
    "MAX_TRAIN_CELLS = 300_000\n",
    "MAX_CELLS_PER_SAMPLE = 5_000\n",
    "\n",
    "# Feature engineering defaults (if missing)\n",
    "ASINH_COFACTOR = 1000.0\n",
    "LOG_SHIFT = 1.0\n",
    "\n",
    "# ensure output dirs\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def list_parquets(data_dir):\n",
    "    return sorted(glob.glob(os.path.join(data_dir, \"*.parquet\")))\n",
    "\n",
    "def drop_rows_with_nonfinite_numeric(df):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(num_cols) == 0:\n",
    "        return df.copy(), 0\n",
    "    mask = np.isfinite(df[num_cols].values).all(axis=1)\n",
    "    removed = (~mask).sum()\n",
    "    return df.loc[mask].copy(), int(removed)\n",
    "\n",
    "def safe_add_feature_columns(df):\n",
    "    \"\"\"\n",
    "    Add basic derived features if they aren't present:\n",
    "     - FITC_raw (from FITC-A or B530-A)\n",
    "     - FITC_log10\n",
    "     - FITC_asinh\n",
    "     - FITC_pct (rank pct)\n",
    "     - FSC_ratio = FSC-A / FSC-H\n",
    "     - SSC_ratio = SSC-A / SSC-H\n",
    "    Operates in-place on a copy and returns df.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # detect FITC column\n",
    "    fitc_candidates = ['FITC-A', 'B530-A', 'FITC_raw', 'FITC']\n",
    "    fitc_col = None\n",
    "    for c in fitc_candidates:\n",
    "        if c in df.columns:\n",
    "            fitc_col = c\n",
    "            break\n",
    "    # if FITC not found, try FITC-A with case-insensitive search\n",
    "    if fitc_col is None:\n",
    "        for c in df.columns:\n",
    "            if str(c).lower().startswith('fitc') or 'b530' in str(c).lower():\n",
    "                fitc_col = c\n",
    "                break\n",
    "\n",
    "    if fitc_col is not None:\n",
    "        df['FITC_raw'] = pd.to_numeric(df[fitc_col], errors='coerce').astype(float)\n",
    "    elif 'FITC_raw' not in df.columns:\n",
    "        df['FITC_raw'] = np.nan\n",
    "\n",
    "    # Clean raw: clip negatives to 0\n",
    "    df['FITC_raw'] = df['FITC_raw'].where(df['FITC_raw'] >= 0.0, other=0.0)\n",
    "\n",
    "    # log10 + shift\n",
    "    if 'FITC_log10' not in df.columns:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df['FITC_log10'] = np.log10(df['FITC_raw'].fillna(0.0) + float(LOG_SHIFT))\n",
    "    # asinh\n",
    "    if 'FITC_asinh' not in df.columns:\n",
    "        df['FITC_asinh'] = np.arcsinh(df['FITC_raw'].fillna(0.0) / float(ASINH_COFACTOR))\n",
    "    # pct rank within sample (works per DataFrame)\n",
    "    if 'FITC_pct' not in df.columns:\n",
    "        try:\n",
    "            df['FITC_pct'] = df['FITC_log10'].rank(pct=True, na_option='bottom')\n",
    "        except Exception:\n",
    "            df['FITC_pct'] = np.nan\n",
    "\n",
    "    # ratios\n",
    "    eps = 1e-9\n",
    "    if 'FSC-A' in df.columns and 'FSC-H' in df.columns and 'FSC_ratio' not in df.columns:\n",
    "        df['FSC_ratio'] = pd.to_numeric(df['FSC-A'], errors='coerce') / (pd.to_numeric(df['FSC-H'], errors='coerce') + eps)\n",
    "    if 'SSC-A' in df.columns and 'SSC-H' in df.columns and 'SSC_ratio' not in df.columns:\n",
    "        df['SSC_ratio'] = pd.to_numeric(df['SSC-A'], errors='coerce') / (pd.to_numeric(df['SSC-H'], errors='coerce') + eps)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# 1) Merge (stream) parquet files into merged.parquet\n",
    "# -----------------------\n",
    "files = list_parquets(DATA_DIR)\n",
    "if len(files) == 0:\n",
    "    print(\"No parquet files found in\", DATA_DIR)\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"Found {len(files)} parquet files. Merging into {MERGED_PATH} ...\")\n",
    "# use first non-empty file to obtain schema\n",
    "first_schema = None\n",
    "writer = None\n",
    "rows_written = 0\n",
    "total_removed = 0\n",
    "total_rows = 0\n",
    "\n",
    "for p in tqdm(files, desc=\"Merging files\"):\n",
    "    try:\n",
    "        df = pd.read_parquet(p)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: cannot read {p}: {e}\")\n",
    "        continue\n",
    "    total_rows += len(df)\n",
    "    df_clean, removed = drop_rows_with_nonfinite_numeric(df)\n",
    "    total_removed += removed\n",
    "    if df_clean.empty:\n",
    "        continue\n",
    "\n",
    "    # ensure some derived features exist (so schema is consistent)\n",
    "    df_clean = safe_add_feature_columns(df_clean)\n",
    "\n",
    "    # set schema from first written\n",
    "    if writer is None:\n",
    "        table0 = pa.Table.from_pandas(df_clean, safe=False)\n",
    "        schema = table0.schema\n",
    "        writer = pq.ParquetWriter(MERGED_PATH, schema, compression='snappy', use_dictionary=True)\n",
    "    # align columns to schema (add missing as NaN)\n",
    "    schema_cols = [f.name for f in schema]\n",
    "    for c in schema_cols:\n",
    "        if c not in df_clean.columns:\n",
    "            df_clean[c] = np.nan\n",
    "    # reindex into same order\n",
    "    df_to_write = df_clean[schema_cols]\n",
    "    writer.write_table(pa.Table.from_pandas(df_to_write, schema=schema, safe=False))\n",
    "    rows_written += len(df_to_write)\n",
    "\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"Merging finished. total rows scanned: {total_rows}, rows written: {rows_written}, rows removed: {total_removed}\")\n",
    "if rows_written == 0:\n",
    "    print(\"No rows written to merged parquet. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Inspect merged schema and decide training branches\n",
    "# -----------------------\n",
    "pf = pq.ParquetFile(MERGED_PATH)\n",
    "merged_columns = pf.schema_arrow.names\n",
    "print(\"Merged columns (sample):\", merged_columns[:40])\n",
    "has_label = 'label' in merged_columns\n",
    "print(\"Cell-level label present?:\", has_label)\n",
    "\n",
    "# -----------------------\n",
    "# 3) Build cell-level dataset (subsample) and train classifier if labels exist\n",
    "# -----------------------\n",
    "cell_model = None\n",
    "cell_feature_list = None\n",
    "\n",
    "if has_label:\n",
    "    print(\"Building cell-level training dataset (subsampling across files)...\")\n",
    "    collected = 0\n",
    "    frames = []\n",
    "    for p in tqdm(files, desc=\"Sampling files for cell-level\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(p)\n",
    "        except Exception:\n",
    "            continue\n",
    "        df_clean, removed = drop_rows_with_nonfinite_numeric(df)\n",
    "        if df_clean.empty or 'label' not in df_clean.columns:\n",
    "            continue\n",
    "        # ensure derived features exist\n",
    "        df_clean = safe_add_feature_columns(df_clean)\n",
    "        n = len(df_clean)\n",
    "        take = min(n, MAX_CELLS_PER_SAMPLE)\n",
    "        remaining = MAX_TRAIN_CELLS - collected\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        take = min(take, remaining)\n",
    "        sample = df_clean.sample(n=take, random_state=RANDOM_STATE) if take < n else df_clean\n",
    "        frames.append(sample)\n",
    "        collected += len(sample)\n",
    "    if collected == 0:\n",
    "        print(\"No labeled cells found after cleaning. Skipping cell-level training.\")\n",
    "    else:\n",
    "        cell_df = pd.concat(frames, ignore_index=True)\n",
    "        # choose numeric features except label\n",
    "        num_cols = cell_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if 'label' in num_cols:\n",
    "            num_cols.remove('label')\n",
    "        # drop any columns with all-NaN\n",
    "        num_cols = [c for c in num_cols if not cell_df[c].isna().all()]\n",
    "        X = cell_df[num_cols].astype(float).fillna(0.0).values\n",
    "        y = cell_df['label'].astype(int).values\n",
    "        print(\"Cell-level dataset shape:\", X.shape, \"positive rate:\", np.mean(y))\n",
    "        # split, train\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "        clf = RandomForestClassifier(n_estimators=300, class_weight='balanced', n_jobs=-1, random_state=RANDOM_STATE)\n",
    "        clf.fit(Xtr, ytr)\n",
    "        yp = clf.predict(Xte)\n",
    "        yprob = clf.predict_proba(Xte)[:, 1] if hasattr(clf, \"predict_proba\") else None\n",
    "        print(\"Cell-level accuracy:\", accuracy_score(yte, yp))\n",
    "        try:\n",
    "            print(\"Cell-level ROC AUC:\", roc_auc_score(yte, yprob))\n",
    "        except Exception:\n",
    "            pass\n",
    "        # save\n",
    "        joblib.dump({'model': clf, 'features': num_cols}, CELL_MODEL_PATH)\n",
    "        print(\"Saved cell-level model to\", CELL_MODEL_PATH)\n",
    "        cell_model = clf\n",
    "        cell_feature_list = num_cols\n",
    "\n",
    "# -----------------------\n",
    "# 4) Build sample-level summary (per-file aggregation) and train regressor\n",
    "# -----------------------\n",
    "print(\"Building sample-level summary from each original parquet file...\")\n",
    "summary_rows = []\n",
    "for p in tqdm(files, desc=\"Computing sample summaries\"):\n",
    "    try:\n",
    "        df = pd.read_parquet(p)\n",
    "    except Exception:\n",
    "        continue\n",
    "    df_clean, removed = drop_rows_with_nonfinite_numeric(df)\n",
    "    if df_clean.empty:\n",
    "        continue\n",
    "    # make sure features exist\n",
    "    df_clean = safe_add_feature_columns(df_clean)\n",
    "    row = {'sample_file': os.path.basename(p), 'n_cells': len(df_clean)}\n",
    "    if 'label' in df_clean.columns:\n",
    "        row['pct_infected_true'] = 100.0 * df_clean['label'].astype(bool).mean()\n",
    "    else:\n",
    "        row['pct_infected_true'] = np.nan\n",
    "    num_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for c in num_cols:\n",
    "        row[f'{c}_median'] = float(df_clean[c].median()) if df_clean[c].dropna().size>0 else np.nan\n",
    "        row[f'{c}_mean'] = float(df_clean[c].mean()) if df_clean[c].dropna().size>0 else np.nan\n",
    "        row[f'{c}_p90'] = float(df_clean[c].quantile(0.90)) if df_clean[c].dropna().size>0 else np.nan\n",
    "    summary_rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_parquet(SUMMARY_PATH, index=False)\n",
    "print(\"Saved sample summary to\", SUMMARY_PATH)\n",
    "\n",
    "# Train sample-level regressor if enough true labels exist\n",
    "n_with_target = summary_df['pct_infected_true'].notna().sum()\n",
    "print(\"Samples with true target:\", n_with_target)\n",
    "sample_model = None\n",
    "sample_feature_list = None\n",
    "scaler = None\n",
    "\n",
    "if n_with_target >= 10:\n",
    "    df_train = summary_df.dropna(subset=['pct_infected_true']).copy()\n",
    "    # choose numeric features excluding target\n",
    "    feat_candidates = [c for c in df_train.columns if c not in ['sample_file', 'pct_infected_true'] and np.issubdtype(df_train[c].dtype, np.number)]\n",
    "    # drop columns with too many NaN\n",
    "    completeness = df_train[feat_candidates].notna().mean(axis=0)\n",
    "    # keep cols with >= 0.5 completeness (adjustable)\n",
    "    keep = completeness[completeness >= 0.5].index.tolist()\n",
    "    if len(keep) < 5:\n",
    "        keep = completeness.sort_values(ascending=False).head(10).index.tolist()\n",
    "    X_df = df_train[keep].apply(pd.to_numeric, errors='coerce')\n",
    "    # impute NaNs with median\n",
    "    X_df = X_df.fillna(X_df.median())\n",
    "    y = df_train['pct_infected_true'].values.astype(float)\n",
    "    # scale features (robust)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X_df.values)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_scaled, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    rfr = RandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "    rfr.fit(Xtr, ytr)\n",
    "    yp = rfr.predict(Xte)\n",
    "    print(\"Sample-level MAE:\", mean_absolute_error(yte, yp))\n",
    "    print(\"Sample-level R2:\", r2_score(yte, yp))\n",
    "    joblib.dump({'model': rfr, 'features': keep, 'scaler': scaler}, SAMPLE_MODEL_PATH)\n",
    "    print(\"Saved sample-level model to\", SAMPLE_MODEL_PATH)\n",
    "    sample_model = rfr\n",
    "    sample_feature_list = keep\n",
    "else:\n",
    "    print(\"Not enough samples with true target to train sample-level regressor. Skipping training.\")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Inference: apply models to each file and write Excel + plots\n",
    "# -----------------------\n",
    "print(\"Running inference over all files and producing Excel + plots...\")\n",
    "out_rows = []\n",
    "for p in tqdm(files, desc=\"Running inference\"):\n",
    "    try:\n",
    "        df = pd.read_parquet(p)\n",
    "    except Exception:\n",
    "        continue\n",
    "    df_clean, removed = drop_rows_with_nonfinite_numeric(df)\n",
    "    if df_clean.empty:\n",
    "        continue\n",
    "    df_clean = safe_add_feature_columns(df_clean)\n",
    "    row = {'sample_file': os.path.basename(p), 'n_cells_clean': len(df_clean)}\n",
    "    # cell-level prediction: proportion predicted infected by cell_model (if exists)\n",
    "    if cell_model is not None and cell_feature_list is not None:\n",
    "        # ensure columns exist\n",
    "        missing = [c for c in cell_feature_list if c not in df_clean.columns]\n",
    "        if missing:\n",
    "            # create missing with NaN\n",
    "            for c in missing:\n",
    "                df_clean[c] = np.nan\n",
    "        Xc = df_clean[cell_feature_list].apply(pd.to_numeric, errors='coerce').fillna(0.0).values\n",
    "        try:\n",
    "            probs = cell_model.predict_proba(Xc)[:, 1]\n",
    "            row['cell_model_pct_infected'] = 100.0 * float(np.mean(probs))\n",
    "        except Exception:\n",
    "            row['cell_model_pct_infected'] = np.nan\n",
    "    else:\n",
    "        row['cell_model_pct_infected'] = np.nan\n",
    "\n",
    "    # sample-level model prediction from aggregated features\n",
    "    if sample_model is not None and sample_feature_list is not None and scaler is not None:\n",
    "        # build feature vector from medians (as used in training)\n",
    "        v = {}\n",
    "        for c in sample_feature_list:\n",
    "            # original training used medians/means; choose medians if present\n",
    "            base_col = c.replace('_median','').replace('_mean','')\n",
    "            # try to find the column in df_clean that corresponds to base_col\n",
    "            if base_col in df_clean.columns:\n",
    "                v[c] = df_clean[base_col].median()\n",
    "            else:\n",
    "                # fallback: try direct column name (maybe training used the summary col name)\n",
    "                v[c] = np.nan\n",
    "        xf = pd.Series(v).astype(float).fillna(pd.Series(v).median())\n",
    "        Xs = scaler.transform(xf.values.reshape(1, -1))\n",
    "        try:\n",
    "            pred = sample_model.predict(Xs)[0]\n",
    "            row['sample_model_pred_pct'] = float(pred)\n",
    "        except Exception:\n",
    "            row['sample_model_pred_pct'] = np.nan\n",
    "    else:\n",
    "        row['sample_model_pred_pct'] = np.nan\n",
    "\n",
    "    # truth (if exists)\n",
    "    if 'label' in df_clean.columns:\n",
    "        row['true_pct_infected'] = float(100.0 * df_clean['label'].astype(bool).mean())\n",
    "    else:\n",
    "        row['true_pct_infected'] = np.nan\n",
    "\n",
    "    out_rows.append(row)\n",
    "\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "# save to excel\n",
    "out_df = out_df.sort_values('sample_file').reset_index(drop=True)\n",
    "out_df.to_excel(EXCEL_OUT, index=False)\n",
    "print(\"Saved Excel summary to\", EXCEL_OUT)\n",
    "\n",
    "# simple plots: true vs predicted scatter (if available)\n",
    "try:\n",
    "    plt.figure(figsize=(6,6))\n",
    "    mask = out_df['true_pct_infected'].notna() & out_df['sample_model_pred_pct'].notna()\n",
    "    if mask.sum() > 0:\n",
    "        plt.scatter(out_df.loc[mask,'true_pct_infected'], out_df.loc[mask,'sample_model_pred_pct'], s=8)\n",
    "        plt.plot([0,100],[0,100],'k--',linewidth=0.8)\n",
    "        plt.xlabel('True % infected')\n",
    "        plt.ylabel('Predicted % infected')\n",
    "        plt.title('Sample-level: true vs predicted')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR,'sample_true_vs_pred.png'), dpi=150)\n",
    "        plt.close()\n",
    "    # histogram of predicted cell-level %\n",
    "    if 'cell_model_pct_infected' in out_df.columns and out_df['cell_model_pct_infected'].notna().sum() > 0:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        out_df['cell_model_pct_infected'].dropna().hist(bins=50)\n",
    "        plt.xlabel('Cell-model % infected')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR,'cell_model_pct_hist.png'), dpi=150)\n",
    "        plt.close()\n",
    "    print(\"Saved plots to\", PLOTS_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Plotting failed:\", e)\n",
    "\n",
    "print(\"Pipeline finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8761343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
